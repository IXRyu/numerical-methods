{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0700449d",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d42826",
   "metadata": {},
   "source": [
    "Consider the following document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8515c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.CountVectorizer(min_df=1)\n",
    "\n",
    "documents = [\n",
    "    'The rank of a matrix is the maximum number of linearly independent columns.',\n",
    "    'The graph of a function in two variable is a surface in the 3D space',\n",
    "    'The partial derivative is the ordinary derivative of a partial function',\n",
    "    'The eigenvalues of a matrix are the roots of the characteristic polynomial.',       \n",
    "    'The gradient of a two-variables function contains its partial derivatives',\n",
    "    'The partial derivatives are the slope of the tangent plane',\n",
    "    'If a function is convex then any local minimizer is a global minimizer',\n",
    "    'A steepest descent method uses the opposite of the gradient direction',\n",
    "    'The steepest descent method is globally convergent if the Wolfe condition holds',\n",
    "    'The Hessian matrix is the matrix of second-order partial derivatives',\n",
    "    'The inverse of an orthogonal matrix is its transpose.',\n",
    "    'If A is a singular matrix then it has at least one null eigenvalue.',\n",
    "    'The product of an orthogonal matrix and its traspose is the identity matrix.',\n",
    "    'The tangent plane is the linear approximation of a surface',\n",
    "    'The column space of a matrix A is called range of A.',\n",
    "    'The rank of a matrix is the maximum number of linearly independent rows.',\n",
    "    'A set of orthogonal vectors is a linearly independent set.',\n",
    "    'The spectrum of a matrix is the set of all its distinct eigenvalues.',\n",
    "    'The columns of an orthogonal matrix are a set of orthogonal vectors.',\n",
    "    'A normed vectorial space is a space with an inner product norm.',\n",
    "    'Similar matrices have the same spectrum.',\n",
    "    'The Hessian matrix of a convex function is positive semi-definite',\n",
    "    'Matrices not similar to a diagonal matrix are called defective.',\n",
    "    'The gradient of a stationary point is the zero vector',\n",
    "    'A symmetric matrix has real eigenvalues and orthogonal eigenvectors.',\n",
    "    'The determinant of a matrix represents the volume scaling factor of the linear transformation.',\n",
    "    'A matrix is invertible if and only if its determinant is nonzero.',\n",
    "    'The null space of a matrix contains all vectors mapped to zero.',\n",
    "    'An orthonormal basis consists of orthogonal unit vectors.',\n",
    "    'The trace of a matrix is the sum of its diagonal elements.',\n",
    "    'Positive definite matrices have strictly positive eigenvalues.',\n",
    "    'The Jacobian matrix generalizes the gradient to vector-valued functions.',\n",
    "    'A linear transformation preserves vector addition and scalar multiplication.',\n",
    "    'The condition number of a matrix measures the sensitivity of the solution of a linear system.'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f22e66",
   "metadata": {},
   "source": [
    "To remove the stop words and perform stemming you can download the package \"gensim\" by uncommenting the following instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afa7f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\fabio\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\fabio\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.15.3)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\fabio\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from smart_open>=1.8.1->gensim) (1.17.2)\n",
      "Downloading gensim-4.4.0-cp310-cp310-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 4.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.6/24.4 MB 4.7 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.8/24.4 MB 5.0 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 2.6/24.4 MB 932.2 kB/s eta 0:00:24\n",
      "   ------ --------------------------------- 3.9/24.4 MB 1.3 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 2.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 8.7/24.4 MB 2.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 12.8/24.4 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.5/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 4.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.4/24.4 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.3/24.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 5.0 MB/s  0:00:04\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   ---------------------------------------- 2/2 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000177E4CE2950>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/53/fe/e483909cfbfa8cc4bfd30aa9fb5170c04316cc22f23c9906529f08fb9095/gensim-4.4.0-cp310-cp310-win_amd64.whl.metadata\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad73cc18",
   "metadata": {},
   "source": [
    "Then, after you installed \"gensim\" you can proceed with the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf53cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, stem_text\n",
    "import gensim.parsing.preprocessing  as text_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9060bfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Documents:\n",
      "- rank matrix maximum number linearli independ column\n",
      "- graph function variabl surfac 3d space\n",
      "- partial deriv ordinari deriv partial function\n",
      "- eigenvalu matrix root characterist polynomi\n",
      "- gradient two variabl function contain partial deriv\n",
      "- partial deriv slope tangent plane\n",
      "- function convex local minim global minim\n",
      "- steepest descent method us opposit gradient direct\n",
      "- steepest descent method global converg wolf condit hold\n",
      "- hessian matrix matrix second order partial deriv\n",
      "- invers orthogon matrix transpos\n",
      "- singular matrix null eigenvalu\n",
      "- product orthogon matrix traspos ident matrix\n",
      "- tangent plane linear approxim surfac\n",
      "- column space matrix call rang a\n",
      "- rank matrix maximum number linearli independ row\n",
      "- set orthogon vector linearli independ set\n",
      "- spectrum matrix set distinct eigenvalu\n",
      "- column orthogon matrix set orthogon vector\n",
      "- norm vectori space space inner product norm\n",
      "- similar matric spectrum\n",
      "- hessian matrix convex function posit semi definit\n",
      "- matric similar diagon matrix call defect\n",
      "- gradient stationari point zero vector\n",
      "- symmetr matrix real eigenvalu orthogon eigenvector\n",
      "- determin matrix repres volum scale factor linear transform\n",
      "- matrix invert determin nonzero\n",
      "- null space matrix contain vector map zero\n",
      "- orthonorm basi consist orthogon unit vector\n",
      "- trace matrix sum diagon element\n",
      "- posit definit matric strictli posit eigenvalu\n",
      "- jacobian matrix gener gradient vector valu function\n",
      "- linear transform preserv vector addit scalar multipl\n",
      "- condit number matrix measur sensit solut linear system\n"
     ]
    }
   ],
   "source": [
    "def preprocess_doc(doc):\n",
    "    doc = doc.lower()   # Gensim does not recognize stopword if uppercase, so bring all to lowercase \n",
    "    doc = text_pre.remove_stopwords(doc)\n",
    "    # While not specified in the trace puntuation interfere with stem_text\n",
    "    doc = text_pre.strip_punctuation(doc)\n",
    "    doc = text_pre.stem_text(doc)   \n",
    "    return doc\n",
    "\n",
    "# Run preprocessing function on each document\n",
    "prep_doc = list(map(preprocess_doc, documents))\n",
    "\n",
    "print(\"Preprocessed Documents:\")\n",
    "for d in prep_doc:\n",
    "    print(\"-\",d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c697cbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer.vocabulary_: {'rank': 65, 'matrix': 42, 'maximum': 43, 'number': 51, 'linearli': 38, 'independ': 32, 'column': 6, 'graph': 28, 'function': 24, 'variabl': 98, 'surfac': 86, '3d': 0, 'space': 80, 'partial': 57, 'deriv': 14, 'ordinari': 54, 'eigenvalu': 20, 'root': 68, 'characterist': 5, 'polynomi': 60, 'gradient': 27, 'two': 94, 'contain': 9, 'slope': 78, 'tangent': 89, 'plane': 58, 'convex': 11, 'local': 39, 'minim': 46, 'global': 26, 'steepest': 83, 'descent': 15, 'method': 45, 'us': 96, 'opposit': 52, 'direct': 18, 'converg': 10, 'wolf': 102, 'condit': 7, 'hold': 30, 'hessian': 29, 'second': 72, 'order': 53, 'invers': 34, 'orthogon': 55, 'transpos': 92, 'singular': 77, 'null': 50, 'product': 63, 'traspos': 93, 'ident': 31, 'linear': 37, 'approxim': 2, 'call': 4, 'rang': 64, 'row': 69, 'set': 75, 'vector': 99, 'spectrum': 81, 'distinct': 19, 'norm': 49, 'vectori': 100, 'inner': 33, 'similar': 76, 'matric': 41, 'posit': 61, 'semi': 73, 'definit': 13, 'diagon': 17, 'defect': 12, 'stationari': 82, 'point': 59, 'zero': 103, 'symmetr': 87, 'real': 66, 'eigenvector': 21, 'determin': 16, 'repres': 67, 'volum': 101, 'scale': 71, 'factor': 23, 'transform': 91, 'invert': 35, 'nonzero': 48, 'map': 40, 'orthonorm': 56, 'basi': 3, 'consist': 8, 'unit': 95, 'trace': 90, 'sum': 85, 'element': 22, 'strictli': 84, 'jacobian': 36, 'gener': 25, 'valu': 97, 'preserv': 62, 'addit': 1, 'scalar': 70, 'multipl': 47, 'measur': 44, 'sensit': 74, 'solut': 79, 'system': 88}\n",
      "(104, 34)\n"
     ]
    }
   ],
   "source": [
    "Y = vectorizer.fit_transform(prep_doc).toarray()\n",
    "print('vectorizer.vocabulary_: {0}'.format(vectorizer.vocabulary_))\n",
    "\n",
    "A = Y.T\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfec48",
   "metadata": {},
   "source": [
    "Consider the query vector ``gradient''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8265ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    " #query vector\n",
    "query1text = ['gradient']\n",
    "query_stem = query1text[0]#stem_text(query1text[0])\n",
    "query1 = []\n",
    "query1.append(query_stem)\n",
    "query1 = vectorizer.transform(query1).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a34ec9",
   "metadata": {},
   "source": [
    "As you can see the vector ``query1`` is a row vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78ca486b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 104)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query1.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e483c02",
   "metadata": {},
   "source": [
    "The search for relevant documents is carried out by computing the cosines of the angles  between the query\n",
    "vector and the document vectors. A document is returned as relevant only if the cosine of such an angle is greater than some threshold or cutoff value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef6c26e",
   "metadata": {},
   "source": [
    "Compute:\n",
    "\n",
    "- The \"cosine similarity\" between the query vector ``query1`` and the columns of $A$\n",
    "- The \"cosine similarity\" between the query vector ``query1`` and an orthogonal basis of the column space of $A$ (use the QR factorization with pivot: ``[Q,R,P]=scipy.linalg.qr(A,mode='economic',pivoting=True)`` ). In particular derermine a suitable subspace spanned by $k$ elements rather than the full set of $rank(A)$ elements by setting the tolerance to ``0.92`` and choosing $k$ following these instructions:\n",
    " \n",
    "  1) extract the diagonal elements of ``R`` and copy them in an auxiliary vector ``Rdiag``;\n",
    "    \n",
    "  2) scale the absolute values of ``Rdiag`` with respect to its absolute maximum\n",
    "    \n",
    "  3) compute $k$ as  the number of elements of ``Rdiag`` that are greather then the chosen tolerance\n",
    "  \n",
    "- Then perform the Latent Semantic Index to compute the cosine of the angles. Choose one of the showed techniques for computing  a suitable number $k$ of components. \n",
    "Show the error with respect to the cosine similarity computed using the full matriz $A$ and discuss the obtained results.\n",
    "\n",
    "- Compute using the singular value decomposition the closest point in the range of $A$ and in the range of $A_k$ to the query. \n",
    "\n",
    "- Choose another query vector ``query2`` at your discretion and repeat the previous steps. \n",
    "\n",
    "- Construct the column-covariance matrix of the original matrix $A$ (do not subtract the means of the columns), numerically determine if it is positive definite. Do the eigenvalues of such a matrix satisfy the theoretical relation with the singular value of the matrix $A$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c69b8",
   "metadata": {},
   "source": [
    "## Resolution of exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8381a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\fabio\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\fabio\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c737327",
   "metadata": {},
   "source": [
    "### Point 1: \n",
    "The \"cosine similarity\" between the query vector ``query1`` and the columns of $A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7729857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between query1 and the columns of A\n",
      "Values: [[0.         0.         0.         0.         0.37796447 0.\n",
      "  0.         0.37796447 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.        ]] \n",
      "\n",
      "Non-zero similarity documents:\n",
      "  Document 5: 0.377964\n",
      "  Document 8: 0.377964\n",
      "  Document 24: 0.447214\n",
      "  Document 32: 0.377964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(query1, A.T)\n",
    "print(f\"Similarity between query1 and the columns of A\")\n",
    "print(f\"Values: {similarity} \\n\")\n",
    "#here i noticed that it's sparse, so from now on i'll only print the non-zero values if it's sparse\n",
    "print(f\"Non-zero similarity documents:\")\n",
    "for i, sim in enumerate(similarity[0]):\n",
    "    if sim > 0:\n",
    "        print(f\"Document {i+1}: {sim:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f5956",
   "metadata": {},
   "source": [
    "#### Discussion Point 1:\n",
    "\n",
    "The cosine similarity measures the angle between two vectors, with values ranging from -1 to 1:\n",
    "- **1** means vectors point in the same direction (identical)\n",
    "- **0** means vectors are orthogonal (no similarity)\n",
    "- **-1** means vectors point in opposite directions\n",
    "\n",
    "For the query \"gradient\", we found non-zero similarity only with documents that contain this word:\n",
    "- **Document 5**: \"The gradient of a two-variables function contains its partial derivatives\"\n",
    "- **Document 8**: \"A steepest descent method uses the opposite of the gradient direction\"\n",
    "- **Document 24**: \"The gradient of a stationary point is the zero vector\"\n",
    "- **Document 32**: \"The Jacobian matrix generalizes the gradient to vector-valued functions\"\n",
    "\n",
    "The similarity values vary depending on the document length and other terms present. This is the **baseline** similarity that we will compare against the LSI approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06716272",
   "metadata": {},
   "source": [
    "### Point 2:\n",
    "- The \"cosine similarity\" between the query vector ``query1`` and an orthogonal basis of the column space of $A$ (use the QR factorization with pivot: ``[Q,R,P]=scipy.linalg.qr(A,mode='economic',pivoting=True)`` ). In particular derermine a suitable subspace spanned by $k$ elements rather than the full set of $rank(A)$ elements by setting the tolerance to ``0.92`` and choosing $k$ following these instructions:\n",
    " \n",
    "  1) extract the diagonal elements of ``R`` and copy them in an auxiliary vector ``Rdiag``;\n",
    "    \n",
    "  2) scale the absolute values of ``Rdiag`` with respect to its absolute maximum\n",
    "    \n",
    "  3) compute $k$ as  the number of elements of ``Rdiag`` that are greather then the chosen tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82dc51a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR decomposition. Shapes Q: (104, 34), R: (34, 34), P: (34,)\n",
      "Non-zero similarity between query1 and the columns of Q\n",
      "  Column 12: 0.0179\n",
      "  Column 13: 0.0902\n",
      "  Column 16: 0.0238\n",
      "  Column 17: 0.0271\n",
      "  Column 19: 0.0031\n",
      "  Column 24: 0.3293\n",
      "  Column 26: 0.0472\n",
      "  Column 29: 0.0147\n",
      "  Column 34: 0.0107\n",
      "\n",
      "rank k: 2\n"
     ]
    }
   ],
   "source": [
    "from scipy import linalg\n",
    "[Q,R,P]=linalg.qr(A,mode='economic',pivoting=True)\n",
    "\n",
    "sim = cosine_similarity(query1, Q.T)\n",
    "print(f\"Non-zero similarity between query1 and the columns of Q\")\n",
    "for i, s in enumerate(sim[0]):\n",
    "    if s > 0:\n",
    "        print(f\"Column {i+1}: {s:.6f}\")\n",
    "#step 1\n",
    "Rdiag = np.diag(R)\n",
    "k = 0\n",
    "#step 2\n",
    "max = np.max(np.abs(Rdiag))\n",
    "tol = 0.92\n",
    "scaled_Rdiag = np.abs(Rdiag) / max\n",
    "#step 3\n",
    "for element in scaled_Rdiag:\n",
    "    if element > tol:\n",
    "        k += 1\n",
    "print(f\"\\nrank k: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10392e61",
   "metadata": {},
   "source": [
    "#### Discussion Point 2:\n",
    "\n",
    "**QR Factorization with Column Pivoting** produces $A \\cdot P = Q \\cdot R$, where:\n",
    "- $Q$ is orthonormal (columns are orthogonal unit vectors)\n",
    "- $R$ is upper triangular with diagonal elements in decreasing magnitude order (due to pivoting)\n",
    "- $P$ is a permutation matrix\n",
    "\n",
    "The **numerical rank** is determined by finding where the diagonal of $R$ drops below a threshold:\n",
    "$$k = \\max\\{i : |R_{ii}| \\geq \\text{tol} \\cdot |R_{11}|\\}$$\n",
    "\n",
    "With tolerance **0.92**, we obtained **k = 2**, meaning only 2 columns of $A$ are needed to capture the essential structure. This is a very aggressive dimensionality reduction (from 34 to 2 dimensions), indicating:\n",
    "1. The document-term matrix has high redundancy\n",
    "2. Most documents share similar word patterns\n",
    "3. The tolerance 0.92 is quite strict (keeping only columns with diagonal values ≥ 92% of maximum)\n",
    "\n",
    "A lower tolerance would yield a higher rank (more retained dimensions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e457259",
   "metadata": {},
   "source": [
    "### Point 3:\n",
    "Perform the Latent Semantic Index to compute the cosine of the angles. Choose one of the showed techniques for computing  a suitable number $k$ of components. \n",
    "Show the error with respect to the cosine similarity computed using the full matrix $A$ and discuss the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4fccd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSI with k from percentage of variance (k=13): [[-0.06032757 -0.15090039  0.18125114 -0.02230052  0.07631955  0.25140035\n",
      "   0.01425799 -0.02898771  0.14334565 -0.33669904  0.0695019  -0.0016844\n",
      "   0.05186204]]\n",
      "\n",
      "LSI with k from Kaiser Rule (k=14): [[-0.06032757 -0.15090039  0.18125114 -0.02230052  0.07631955  0.25140035\n",
      "   0.01425799 -0.02898771  0.14334565 -0.33669904  0.0695019  -0.0016844\n",
      "   0.05186204 -0.19541158]]\n",
      "\n",
      "LSI with k from Entropy Method (k=24): [[-0.06032757 -0.15090039  0.18125114 -0.02230052  0.07631955  0.25140035\n",
      "   0.01425799 -0.02898771  0.14334565 -0.33669904  0.0695019  -0.0016844\n",
      "   0.05186204 -0.19541158  0.11004667  0.26353273 -0.12591267 -0.13184161\n",
      "  -0.10778365  0.05735522 -0.02174279 -0.04346702 -0.01621578  0.10516381]]\n",
      "\n",
      "LSI Similarity with k from percentage of variance: [[-0.00670947  0.18398599  0.14313976  0.07604498  0.5682407  -0.04855905\n",
      "  -0.01667533  0.66117682  0.24950765  0.06174106 -0.15067422  0.15558776\n",
      "  -0.12052774 -0.20915519 -0.01056548  0.01322334  0.02416171 -0.00000222\n",
      "  -0.0250078  -0.12230729 -0.04589695  0.0531424  -0.02103266  0.85847697\n",
      "  -0.1055457  -0.06143016  0.02509401  0.54659522  0.08504518  0.02587066\n",
      "  -0.04046512  0.74122301  0.16294787 -0.10901237]]\n",
      "\n",
      "LSI Similarity with k from Kaiser Rule: [[ 0.00948128  0.32091211  0.09072485 -0.00945487  0.57816688 -0.04848532\n",
      "  -0.12204196  0.69420259  0.17324228  0.02149256 -0.05056592  0.02906155\n",
      "  -0.04743923 -0.0487606   0.00896656  0.01892611 -0.00273207 -0.0946122\n",
      "   0.00793175 -0.1349799  -0.09730826  0.11122562 -0.03269342  0.73856477\n",
      "  -0.08393633 -0.00419212  0.07205678  0.34507577  0.10160661  0.0239735\n",
      "  -0.02078703  0.74734544  0.0610444  -0.14041505]]\n",
      "\n",
      "LSI Similarity with k from Entropy Method: [[-0.00535824  0.05316414  0.08128733  0.05409158  0.47121163  0.00420993\n",
      "  -0.00492772  0.60542677 -0.02059962 -0.03834797 -0.02111468 -0.08768201\n",
      "   0.01206956 -0.02710898 -0.05984769  0.00555311  0.00244028 -0.04015918\n",
      "   0.0113218  -0.01271884  0.01707961  0.01241422  0.02012513  0.66944114\n",
      "   0.02187526  0.00694161  0.00510186  0.06639591 -0.0339797  -0.01023532\n",
      "  -0.0064129   0.58446752 -0.01175119  0.01093403]]\n",
      "\n",
      "Error in LSI with k from percentage of variance: 1.017681\n",
      "Error in LSI with k from Kaiser Rule: 0.863281\n",
      "Error in LSI with k from Entropy Method: 0.434390\n"
     ]
    }
   ],
   "source": [
    "U, S, VT = linalg.svd(A, full_matrices=False)\n",
    "\n",
    "squared_svalues = S**2\n",
    "#np.sum instead of the \"for\" loop\n",
    "total_variance = np.sum(squared_svalues)\n",
    "\n",
    "#Cumulative percentage of total variation using 70% threshold\n",
    "cumulative_variance = np.cumsum(squared_svalues) / total_variance\n",
    "k_pct = np.where(cumulative_variance >= 0.70)[0][0] + 1\n",
    "\n",
    "#Kaiser Rule\n",
    "k_kaiser = np.sum(S > np.mean(S))\n",
    "\n",
    "#Entropy Method\n",
    "pk = squared_svalues / total_variance\n",
    "entropy = -np.sum(pk * np.log(pk))\n",
    "k_entropy = int(np.ceil(np.exp(entropy)))\n",
    "\n",
    "#query LSI representations with different k\n",
    "Uk_pct = U[:, 0:k_pct]\n",
    "query1_lsi_pct = query1 @ Uk_pct\n",
    "Uk_kaiser = U[:, 0:k_kaiser]\n",
    "query1_lsi_kaiser = query1 @ Uk_kaiser\n",
    "Uk_entropy = U[:, 0:k_entropy]\n",
    "query1_lsi_entropy = query1 @ Uk_entropy\n",
    "\n",
    "print(f\"\\nLSI with k from percentage of variance (k={k_pct}): {query1_lsi_pct}\")\n",
    "print(f\"\\nLSI with k from Kaiser Rule (k={k_kaiser}): {query1_lsi_kaiser}\")\n",
    "print(f\"\\nLSI with k from Entropy Method (k={k_entropy}): {query1_lsi_entropy}\")\n",
    "\n",
    "#Document LSI representations\n",
    "docs_lsi_pct = (Uk_pct.T @ A).T \n",
    "docs_lsi_kaiser = (Uk_kaiser.T @ A).T\n",
    "docs_lsi_entropy = (Uk_entropy.T @ A).T\n",
    "\n",
    "#Cosines with different LSI based on the chosen method\n",
    "lsi_sim_pct = cosine_similarity(query1_lsi_pct, docs_lsi_pct)\n",
    "lsi_sim_kaiser = cosine_similarity(query1_lsi_kaiser, docs_lsi_kaiser)\n",
    "lsi_sim_entropy = cosine_similarity(query1_lsi_entropy, docs_lsi_entropy)\n",
    "print(f\"\\nLSI Similarity with k from percentage of variance: {lsi_sim_pct}\")\n",
    "print(f\"\\nLSI Similarity with k from Kaiser Rule: {lsi_sim_kaiser}\")\n",
    "print(f\"\\nLSI Similarity with k from Entropy Method: {lsi_sim_entropy}\")\n",
    "\n",
    "#Errors relative to original similarity of full matrix A\n",
    "error_pct = np.linalg.norm(similarity - lsi_sim_pct)\n",
    "error_kaiser = np.linalg.norm(similarity - lsi_sim_kaiser)\n",
    "error_entropy = np.linalg.norm(similarity - lsi_sim_entropy)\n",
    "\n",
    "print(f\"\\nError in LSI with k from percentage of variance: {error_pct:.6f}\")\n",
    "print(f\"Error in LSI with k from Kaiser Rule: {error_kaiser:.6f}\")\n",
    "print(f\"Error in LSI with k from Entropy Method: {error_entropy:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25539b",
   "metadata": {},
   "source": [
    "#### Discussion Point 3:\n",
    "\n",
    "**Latent Semantic Indexing (LSI)** uses the truncated SVD $A_k = U_k \\Sigma_k V_k^T$ to create a low-rank approximation. The three k-selection methods give different results:\n",
    "\n",
    "| Method | k value | Criterion |\n",
    "|--------|---------|-----------|\n",
    "| **70% Cumulative Variance** | k_pct | Retain enough singular values so $\\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{i=1}^{n} \\sigma_i^2} \\geq 0.70$ |\n",
    "| **Kaiser Rule** | k_kaiser | Keep singular values where $\\sigma_i^2 > \\bar{\\sigma^2}$ (above mean) |\n",
    "| **Entropy** | k_entropy | Minimize information entropy of the singular value distribution |\n",
    "\n",
    "**Interpretation of Results:**\n",
    "- Smaller k → more aggressive compression → larger approximation error\n",
    "- Larger k → better fidelity → smaller error but less noise filtering\n",
    "- The **error** $\\|similarity - similarity\\_k\\|$ measures how much the similarity ranking changes due to compression\n",
    "\n",
    "LSI often **improves** retrieval by removing noise (synonymy/polysemy effects), so a small positive error might actually yield better semantic matching than the original high-dimensional similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbadc75",
   "metadata": {},
   "source": [
    "### Point 4:\n",
    "Compute using the singular value decomposition the closest point in the range of $A$ and in the range of $A_k$ to the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "089efc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest point in range(A):\n",
      "Norm: 0.725650\n",
      "\n",
      "Closest point in range(Ak):\n",
      "Norm: 0.672934\n",
      "\n",
      "Distance from query1 to range(A): 0.688064\n",
      "Distance from query1 to range(Ak): 0.739702\n"
     ]
    }
   ],
   "source": [
    "#need column vector for projection\n",
    "query1_col = query1.T\n",
    "\n",
    "#Closest point in range(A)\n",
    "#orthogonal projection onto column space of A\n",
    "closest_point_A = U @ (U.T @ query1_col)\n",
    "print(f\"Closest point in range(A):\")\n",
    "print(f\"Norm: {np.linalg.norm(closest_point_A):.6f}\")\n",
    "\n",
    "#Closest point in range(Ak)\n",
    "#k with minimum error(entropy in this case, look point 3)\n",
    "closest_point_Ak = Uk_entropy @ (Uk_entropy.T @ query1_col)\n",
    "print(f\"\\nClosest point in range(Ak):\")\n",
    "print(f\"Norm: {np.linalg.norm(closest_point_Ak):.6f}\")\n",
    "\n",
    "query1_to_A = np.linalg.norm(query1_col - closest_point_A)\n",
    "query1_to_Ak = np.linalg.norm(query1_col - closest_point_Ak)\n",
    "print(f\"\\nDistance from query1 to range(A): {query1_to_A:.6f}\")\n",
    "print(f\"Distance from query1 to range(Ak): {query1_to_Ak:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bde981",
   "metadata": {},
   "source": [
    "#### Discussion Point 4:\n",
    "\n",
    "**Closest Point in Range(A)** - The orthogonal projection of query $q$ onto the column space of $A$:\n",
    "$$q_{proj} = U U^T q$$\n",
    "\n",
    "This gives the point in $\\text{range}(A)$ closest to $q$ in Euclidean norm, where $U$ contains the left singular vectors (orthonormal basis for the column space).\n",
    "\n",
    "**Closest Point in Range(A_k)** - The projection onto the truncated column space:\n",
    "$$q_{proj,k} = U_k U_k^T q$$\n",
    "\n",
    "where $U_k$ contains only the first $k$ columns of $U$.\n",
    "\n",
    "**Geometric Interpretation:**\n",
    "- $\\text{range}(A)$ is a subspace of $\\mathbb{R}^{104}$ (the term space)\n",
    "- The query vector $q$ may not lie in this subspace\n",
    "- The projection finds the closest representable query within the document space\n",
    "- Using the truncated space $\\text{range}(A_k)$ further restricts to the \"semantic\" subspace\n",
    "\n",
    "The difference between the two projections quantifies how much information is lost when reducing from full rank to rank-k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e34c9",
   "metadata": {},
   "source": [
    "### Point 5:\n",
    "Choose another query vector ``query2`` at your discretion and repeat the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4f0f352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query2: 'matrix', shape: (1, 104)\n",
      "\n",
      "Point 1: cosine similarity between query2 and columns of A\n",
      "Document 1: 0.377964\n",
      "Document 4: 0.447214\n",
      "Document 10: 0.666667\n",
      "Document 11: 0.500000\n",
      "Document 12: 0.500000\n",
      "Document 13: 0.707107\n",
      "Document 15: 0.447214\n",
      "Document 16: 0.377964\n",
      "Document 18: 0.447214\n",
      "Document 19: 0.353553\n",
      "Document 22: 0.377964\n",
      "Document 23: 0.408248\n",
      "Document 25: 0.408248\n",
      "Document 26: 0.353553\n",
      "Document 27: 0.500000\n",
      "Document 28: 0.377964\n",
      "Document 30: 0.447214\n",
      "Document 32: 0.377964\n",
      "Document 34: 0.353553\n",
      "\n",
      "Point 2: non-zero similarity between query2 and the columns of Q\n",
      "Column 10: 0.053742\n",
      "Column 14: 0.020752\n",
      "Column 17: 0.075720\n",
      "Column 18: 0.294214\n",
      "Column 20: 0.017418\n",
      "Column 21: 0.058298\n",
      "Column 22: 0.083732\n",
      "Column 23: 0.097189\n",
      "Column 25: 0.052465\n",
      "Column 26: 0.076600\n",
      "Column 28: 0.085243\n",
      "Column 29: 0.069500\n",
      "Column 32: 0.006412\n",
      "Column 33: 0.057845\n",
      "\n",
      "Rank k: 2\n",
      "\n",
      "Point 3: Latent Semantic Indexing\n",
      "\n",
      "LSI Similarity with k from percentage of variance: [[ 0.454995   -0.01965026  0.0255724   0.71642742  0.03452036 -0.03831166\n",
      "   0.0116524  -0.00183738 -0.0099975   0.79435109  0.78568662  0.76675303\n",
      "   0.9007612  -0.06684556  0.71602055  0.43660666 -0.00831521  0.57861584\n",
      "   0.43154219  0.00679692  0.04121077  0.4575491   0.49812713  0.02320547\n",
      "   0.66698506  0.46837454  0.90894315  0.53109791  0.02220706  0.84317911\n",
      "   0.00561782  0.5007487  -0.02536568  0.4674906 ]]\n",
      "\n",
      "LSI Similarity with k from Kaiser Rule: [[ 0.45518661 -0.00956095  0.02348737  0.69453359  0.0364653  -0.0384409\n",
      "   0.00629442  0.00279469 -0.01242691  0.78845559  0.76543477  0.72365267\n",
      "   0.88863735 -0.05595474  0.71574516  0.43677007 -0.00942574  0.55292754\n",
      "   0.43117768  0.00585856  0.03824228  0.45303896  0.49714048  0.02044089\n",
      "   0.66697346  0.46538144  0.90220749  0.48736235  0.02315195  0.84306337\n",
      "   0.00637449  0.4948288  -0.02846415  0.46279971]]\n",
      "\n",
      "LSI Similarity with k from Entropy Method: [[ 0.43177826 -0.00447381  0.00425257  0.53176596 -0.0009763  -0.00660528\n",
      "  -0.00425158  0.00097826  0.00014318  0.7531289   0.712692    0.669956\n",
      "   0.79966165  0.0053896   0.5059828   0.41619921  0.00736644  0.5156789\n",
      "   0.40667294  0.00146735  0.03258937  0.45462051  0.469279   -0.00369154\n",
      "   0.45828655  0.3957383   0.81634279  0.42524015 -0.00470858  0.49660009\n",
      "  -0.01040432  0.41589316 -0.00358659  0.38793237]]\n",
      "\n",
      "Error with k from percentage of variance (k=13): 0.930609\n",
      "Error with k from Kaiser Rule (k=14): 0.887324\n",
      "Error with k from Entropy Method (k=24): 0.485070\n",
      "\n",
      "Best method for query2: entropy with error 0.485070\n",
      "\n",
      "Point 4: Closest point in range(A) and range(Ak)\n",
      "Closest point in range(A):\n",
      "Norm: 0.918410\n",
      "\n",
      "Closest point in range(Ak) with k=24:\n",
      "Norm: 0.914268\n",
      "\n",
      "Distance from query2 to range(A): 0.395629\n",
      "Distance from query2 to range(Ak): 0.405109\n"
     ]
    }
   ],
   "source": [
    "query2text = ['matrix']\n",
    "query2 = vectorizer.transform(query2text).toarray()\n",
    "print(f\"Query2: '{query2text[0]}', shape: {query2.shape}\")\n",
    "\n",
    "#point 1\n",
    "print(\"\\nPoint 1: cosine similarity between query2 and columns of A\")\n",
    "similarity2 = cosine_similarity(query2, A.T)\n",
    "for i, s in enumerate(similarity2[0]):\n",
    "    if s > 0:\n",
    "        print(f\"Document {i+1}: {s:.6f}\")\n",
    "\n",
    "#point 2\n",
    "print(\"\\nPoint 2: non-zero similarity between query2 and the columns of Q\")\n",
    "sim2 = cosine_similarity(query2, Q.T)\n",
    "for i, s in enumerate(sim2[0]):\n",
    "    if s > 0:\n",
    "        print(f\"Column {i+1}: {s:.6f}\")\n",
    "print(f\"\\nRank k: {k}\")\n",
    "\n",
    "print(\"\\nPoint 3: Latent Semantic Indexing\")\n",
    "#LSI representations\n",
    "query2_lsi_pct = query2 @ Uk_pct\n",
    "query2_lsi_kaiser = query2 @ Uk_kaiser\n",
    "query2_lsi_entropy = query2 @ Uk_entropy\n",
    "\n",
    "#cosine similarities with LSI\n",
    "lsi_sim2_pct = cosine_similarity(query2_lsi_pct, docs_lsi_pct)\n",
    "lsi_sim2_kaiser = cosine_similarity(query2_lsi_kaiser, docs_lsi_kaiser)\n",
    "lsi_sim2_entropy = cosine_similarity(query2_lsi_entropy, docs_lsi_entropy)\n",
    "print(f\"\\nLSI Similarity with k from percentage of variance: {lsi_sim2_pct}\")\n",
    "print(f\"\\nLSI Similarity with k from Kaiser Rule: {lsi_sim2_kaiser}\")\n",
    "print(f\"\\nLSI Similarity with k from Entropy Method: {lsi_sim2_entropy}\")\n",
    "\n",
    "#errors relative to original similarity\n",
    "error2_pct = np.linalg.norm(similarity2 - lsi_sim2_pct)\n",
    "error2_kaiser = np.linalg.norm(similarity2 - lsi_sim2_kaiser)\n",
    "error2_entropy = np.linalg.norm(similarity2 - lsi_sim2_entropy)\n",
    "print(f\"\\nError with k from percentage of variance (k={k_pct}): {error2_pct:.6f}\")\n",
    "print(f\"Error with k from Kaiser Rule (k={k_kaiser}): {error2_kaiser:.6f}\")\n",
    "print(f\"Error with k from Entropy Method (k={k_entropy}): {error2_entropy:.6f}\")\n",
    "\n",
    "#best method\n",
    "errors2 = {'pct': error2_pct, 'kaiser': error2_kaiser, 'entropy': error2_entropy}\n",
    "best2 = min(errors2, key=errors2.get)\n",
    "print(f\"\\nBest method for query2: {best2} with error {errors2[best2]:.6f}\")\n",
    "\n",
    "#point 4\n",
    "print(\"\\nPoint 4: Closest point in range(A) and range(Ak)\")\n",
    "query2_col = query2.T\n",
    "\n",
    "#in range(A)\n",
    "closest_point2_A = U @ (U.T @ query2_col)\n",
    "print(f\"Closest point in range(A):\")\n",
    "print(f\"Norm: {np.linalg.norm(closest_point2_A):.6f}\")\n",
    "\n",
    "#in range(Ak) using entropy k\n",
    "closest_point2_Ak = Uk_entropy @ (Uk_entropy.T @ query2_col)\n",
    "print(f\"\\nClosest point in range(Ak) with k={k_entropy}:\")\n",
    "print(f\"Norm: {np.linalg.norm(closest_point2_Ak):.6f}\")\n",
    "\n",
    "#distances\n",
    "query2_to_A = np.linalg.norm(query2_col - closest_point2_A)\n",
    "query2_to_Ak = np.linalg.norm(query2_col - closest_point2_Ak)\n",
    "print(f\"\\nDistance from query2 to range(A): {query2_to_A:.6f}\")\n",
    "print(f\"Distance from query2 to range(Ak): {query2_to_Ak:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83deaf0",
   "metadata": {},
   "source": [
    "#### Discussion Point 5:\n",
    "\n",
    "**Comparison query1=\"gradient\" vs query2=\"matrix\":**\n",
    "\n",
    "The two queries may yield different behaviors:\n",
    "- **\"gradient\"** is a specialized term appearing in fewer documents → sparse similarity vector\n",
    "- **\"matrix\"** is a more common term in numerical methods → likely more documents match\n",
    "\n",
    "Key observations:\n",
    "1. **Similarity Distribution**: A common word like \"matrix\" will have non-zero similarity with more documents\n",
    "2. **LSI Effect**: The approximation error may differ - common words might be better represented in the low-rank space since they contribute to the principal components\n",
    "3. **Projection Magnitude**: The norm of the projection depends on how well the query aligns with the dominant directions (left singular vectors)\n",
    "\n",
    "This comparison highlights how **query specificity** affects retrieval performance in both exact and LSI-approximated similarity computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf899e7",
   "metadata": {},
   "source": [
    "### Point 6: \n",
    "Construct the column-covariance matrix of the original matrix $A$ (do not subtract the means of the columns), numerically determine if it is positive definite. Do the eigenvalues of such a matrix satisfy the theoretical relation with the singular value of the matrix $A$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0a1a4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column-covariance matrix C = A^T @ A\n",
      "Shape: (34, 34)\n",
      "\n",
      "Eigenvalues of C (sorted descending):\n",
      "[35.23477774 18.28607264 14.16870813 13.04142724 11.61322062 11.34485029\n",
      " 10.60152782  9.73248188  8.49344312  8.23236777  6.54698142  6.21398054\n",
      "  6.11999419  5.83178415  5.24781697  4.88165985  4.82804846  4.50179076\n",
      "  3.97705358  3.78719905  3.59244952  3.53401014  3.01789774  2.92063326\n",
      "  2.71029577  2.50957453  2.36734264  2.23796984  2.1149082   1.81242713\n",
      "  1.52396508  1.44244293  0.89521018  0.6356868 ]\n",
      "\n",
      "Minimum eigenvalue: 0.6356868028\n",
      "C is POSITIVE DEFINITE (all eigenvalues > 0)\n",
      "Relation: eigenvalues(A^T @ A) = σ²\n",
      "\n",
      "Singular values σ of A:\n",
      "[5.93588896 4.27622177 3.76413445 3.61129163 3.40781757 3.36821174\n",
      " 3.25599874 3.11969259 2.91435124 2.8692103  2.55870698 2.4927857\n",
      " 2.4738622  2.41490873 2.29081142 2.20944786 2.19728206 2.12174239\n",
      " 1.99425514 1.94607272 1.89537582 1.87989631 1.73720976 1.70898603\n",
      " 1.64629759 1.58416367 1.53861712 1.49598457 1.4542724  1.34626414\n",
      " 1.23448981 1.20101746 0.94615547 0.79729969]\n",
      "\n",
      "Singular values squared σ²:\n",
      "[35.23477774 18.28607264 14.16870813 13.04142724 11.61322062 11.34485029\n",
      " 10.60152782  9.73248188  8.49344312  8.23236777  6.54698142  6.21398054\n",
      "  6.11999419  5.83178415  5.24781697  4.88165985  4.82804846  4.50179076\n",
      "  3.97705358  3.78719905  3.59244952  3.53401014  3.01789774  2.92063326\n",
      "  2.71029577  2.50957453  2.36734264  2.23796984  2.1149082   1.81242713\n",
      "  1.52396508  1.44244293  0.89521018  0.6356868 ]\n",
      "\n",
      "Eigenvalues of C (first 34 values):\n",
      "[35.23477774 18.28607264 14.16870813 13.04142724 11.61322062 11.34485029\n",
      " 10.60152782  9.73248188  8.49344312  8.23236777  6.54698142  6.21398054\n",
      "  6.11999419  5.83178415  5.24781697  4.88165985  4.82804846  4.50179076\n",
      "  3.97705358  3.78719905  3.59244952  3.53401014  3.01789774  2.92063326\n",
      "  2.71029577  2.50957453  2.36734264  2.23796984  2.1149082   1.81242713\n",
      "  1.52396508  1.44244293  0.89521018  0.6356868 ]\n",
      "\n",
      "Absolute difference |λ(C) - σ²|:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Max difference: 4.80e-14\n",
      "\n",
      "yes, the eigenvalues of A^T @ A are equal to the squared singular values of A\n"
     ]
    }
   ],
   "source": [
    "# Column-covariance matrix (without subtracting means): C = A^T @ A\n",
    "C = A.T @ A\n",
    "print(f\"Column-covariance matrix C = A^T @ A\")\n",
    "print(f\"Shape: {C.shape}\")\n",
    "\n",
    "eigenvalues_C = np.linalg.eigvalsh(C)\n",
    "eigenvalues_C_sorted = np.sort(eigenvalues_C)[::-1] #descending\n",
    "\n",
    "print(f\"\\nEigenvalues of C (sorted descending):\")\n",
    "print(eigenvalues_C_sorted)\n",
    "min_eigenvalue = np.min(eigenvalues_C)\n",
    "print(f\"\\nMinimum eigenvalue: {min_eigenvalue:.10f}\")\n",
    "\n",
    "if min_eigenvalue > 0:\n",
    "    print(\"C is POSITIVE DEFINITE (all eigenvalues > 0)\")\n",
    "elif min_eigenvalue >= 0:\n",
    "    print(\"C is POSITIVE SEMI-DEFINITE (all eigenvalues >= 0, some are zero)\")\n",
    "else:\n",
    "    print(\"C is NOT positive definite (has negative eigenvalues)\")\n",
    "\n",
    "print(\"Relation: eigenvalues(A^T @ A) = σ²\")\n",
    "\n",
    "singular_values_squared = S**2\n",
    "print(f\"\\nSingular values σ of A:\")\n",
    "print(S)\n",
    "print(f\"\\nSingular values squared σ²:\")\n",
    "print(singular_values_squared)\n",
    "\n",
    "print(f\"\\nEigenvalues of C (first {len(S)} values):\")\n",
    "print(eigenvalues_C_sorted[:len(S)])\n",
    "\n",
    "difference = np.abs(eigenvalues_C_sorted[:len(S)] - singular_values_squared)\n",
    "print(f\"\\nAbsolute difference |λ(C) - σ²|:\")\n",
    "print(difference)\n",
    "print(f\"\\nMax difference: {np.max(difference):.2e}\")\n",
    "\n",
    "if np.allclose(eigenvalues_C_sorted[:len(S)], singular_values_squared):\n",
    "    print(\"\\nyes, the eigenvalues of A^T @ A are equal to the squared singular values of A\")\n",
    "else:\n",
    "    print(\"\\nno, the values do not match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168b8ab",
   "metadata": {},
   "source": [
    "#### Discussion Point 6:\n",
    "\n",
    "**Column-Covariance Matrix** $C = A^T A$ (without subtracting column means) is a $34 \\times 34$ symmetric matrix representing document-document covariances.\n",
    "\n",
    "**Positive Definiteness Analysis:**\n",
    "- A matrix is **positive definite** if and only if all eigenvalues $\\lambda_i > 0$\n",
    "- A matrix is **positive semi-definite** if and only if all eigenvalues $\\lambda_i \\geq 0$\n",
    "- For $C = A^T A$, we always have $x^T C x = x^T A^T A x = \\|Ax\\|^2 \\geq 0$, so $C$ is always **positive semi-definite**\n",
    "- $C$ is positive definite only if $A$ has full column rank (all columns are linearly independent)\n",
    "\n",
    "**Fundamental Relationship:**\n",
    "$$\\lambda_i(A^T A) = \\sigma_i^2(A)$$\n",
    "\n",
    "**Proof from SVD:** If $A = U \\Sigma V^T$, then:\n",
    "$$A^T A = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T$$\n",
    "\n",
    "This is the **eigendecomposition** of $A^T A$, where:\n",
    "- **Eigenvalues** = squared singular values ($\\sigma_i^2$)\n",
    "- **Eigenvectors** = right singular vectors (columns of $V$)\n",
    "\n",
    "\n",
    "**np.allclose** is used to compare values keeping floating point arithmetic and numerical instabilities into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2c1447",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d6bcf",
   "metadata": {},
   "source": [
    "# Regression model\n",
    "# Relative location of CT slices on axial axis\n",
    "\n",
    " The data are available at:\n",
    "\n",
    " https://archive.ics.uci.edu/dataset/206/relative+location+of+ct+slices+on+axial+axis\n",
    "\n",
    "The dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body.\n",
    "\n",
    "The data was retrieved from a set of 53500 CT images from 74 different patients (43 male, 31 female).\n",
    "\n",
    "To exstract the data use the panda routines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80d72fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patientId  value0  value1  value2  value3  value4  value5  value6  value7  \\\n",
      "0          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "1          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "2          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "3          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "4          0     0.0     0.0     0.0     0.0     0.0     0.0   -0.25   -0.25   \n",
      "\n",
      "   value8  ...  value375  value376  value377  value378  value379  value380  \\\n",
      "0   -0.25  ...     -0.25  0.980381       0.0       0.0       0.0       0.0   \n",
      "1   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "2   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "3   -0.25  ...     -0.25  0.977008       0.0       0.0       0.0       0.0   \n",
      "4   -0.25  ...     -0.25  0.976833       0.0       0.0       0.0       0.0   \n",
      "\n",
      "   value381  value382  value383  reference  \n",
      "0       0.0     -0.25     -0.25  21.803851  \n",
      "1       0.0     -0.25     -0.25  21.745726  \n",
      "2       0.0     -0.25     -0.25  21.687600  \n",
      "3       0.0     -0.25     -0.25  21.629474  \n",
      "4       0.0     -0.25     -0.25  21.571348  \n",
      "\n",
      "[5 rows x 386 columns]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    " \n",
    "# read the dataset using the compression zip\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/static/public/206/relative+location+of+ct+slices+on+axial+axis.zip',compression='zip')\n",
    " \n",
    "# display dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d4940",
   "metadata": {},
   "source": [
    "We transform the data to a matrix of shape 53500 x 386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f293f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53500, 386)\n"
     ]
    }
   ],
   "source": [
    "Aall=df.to_numpy()\n",
    "print(Aall.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc3913",
   "metadata": {},
   "source": [
    "We add a column of all 1 and we organize the input data by dividing in test set and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64e5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X = np.ones((53500,387))\n",
    "X[:,0:386] = Aall\n",
    "X=np.delete(X,385,1)\n",
    "y = Aall[:,385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0b7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size = .9,\n",
    "    test_size = .1,\n",
    "    random_state = 5,\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c6703",
   "metadata": {},
   "source": [
    "Use the prepared data to solve the regression model with all the studied techniques.\n",
    "Can we use the normal equation and the QR factorization? If the answer is positive compare the condition numbers of the QR methods and the normal equations. What are the results?\n",
    "\n",
    "\n",
    "Use the funcation scipy.linalg.lstsq and check if all the lapack drivers works.\n",
    "Compare the results changing the initial value cond. The results are the same? What about the execution time?\n",
    "\n",
    "Analyze the singular values and check if it is possible to use a principal component regression procedure. Compute the solution using the singular value decomposition. \n",
    "Can you observe a relation in the chosen singular value and the value of cond of the routine lstsq?\n",
    "\n",
    "Perform the same analysis by preprocessing the data in order to have data from a normal distribution with mean zero  and compute the singular value decomposition on this matrix.\n",
    "\n",
    "Check the performance of the method by computing the least square residual for the training set and the testset. The minimum and the maximum values of the predicted error for both, the training set and the testset.\n",
    "\n",
    "Compute the multiple R-squared: R2_train = 1 - sum( (y - yest)**2)/sum( (y-mean(y))**2 where y are the value to predict and yest are the estimated values for the training set.\n",
    "Compute the value R2_test for the testset.\n",
    "\n",
    "A value of R2 near one means that the constructed model is good.\n",
    "\n",
    "Change the size of the training set and the testing set to 0.7% and 0.3% and repeat the previous steps.\n",
    "\n",
    "Comment the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f0268",
   "metadata": {},
   "source": [
    "### Point 1\n",
    "Use the prepared data to solve the regression model with all the studied techniques.\n",
    "Can we use the normal equation and the QR factorization? If the answer is positive compare the condition numbers of the QR methods and the normal equations. What are the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5f6ec-f468-4022-aa52-0bc2f4d61330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of X^T X: 375\n",
      "Shape of X^T X: (386, 386)\n",
      "Full rank required: 386\n",
      "\n",
      "Matrix X^T X is singular, we cannot apply normal equation\n",
      "Matrix shape: 48150 x 386\n",
      "Rank of X: 375\n",
      "Full column rank: 386\n",
      "\n",
      "rank of X < n, using QR with column pivoting matrix\n",
      "Condition number of R: 4.9702e+04\n",
      "\n",
      "Training Results:\n",
      "Residual norm (2-norm): 1.798374e+03\n",
      "Maximum absolute error: 49.471292\n",
      "Minimum absolute error: 0.000000\n",
      "R² score: 0.865383\n",
      "\n",
      "Test Results:\n",
      "Residual norm (2-norm): 6.131967e+02\n",
      "Maximum absolute error: 46.986215\n",
      "Minimum absolute error: 0.002331\n",
      "R² score: 0.860321\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg as linalg\n",
    "\n",
    "def Normal_Equations(X_train, y_train, X_test, y_test):\n",
    "    XtX = X_train.T @ X_train\n",
    "    rank = np.linalg.matrix_rank(XtX)\n",
    "    n = X_train.shape[1]\n",
    "        \n",
    "    print(f\"Rank of X^T X: {rank}\")\n",
    "    print(f\"Shape of X^T X: {XtX.shape}\")\n",
    "    print(f\"Full rank required: {n}\")\n",
    "        \n",
    "    if rank == n:\n",
    "        print(\"\\nMatrix X^T X is non singular, we can apply normal equations\")\n",
    "        \n",
    "        theta = np.linalg.solve(XtX, X_train.T @ y_train)\n",
    "        cond = np.linalg.cond(XtX)\n",
    "        print(f\"Condition number of X^T X: {cond:.4e}\")\n",
    "        y_train_pred = X_train @ theta\n",
    "        residuals_train = y_train - y_train_pred\n",
    "        print(\"\\nTraining Results:\")\n",
    "        print(f\"Residual 2-norm: {np.linalg.norm(residuals_train, 2):.6e}\")\n",
    "        print(f\"Maximum absolute error: {np.max(np.abs(residuals_train)):.6f}\")\n",
    "        print(f\"Minimum absolute error: {np.min(np.abs(residuals_train)):.6f}\")\n",
    "        R2_train = 1 - np.sum((y_train - y_train_pred)**2) / np.sum((y_train - np.mean(y_train))**2)\n",
    "        print(f\"R² score: {R2_train:.6f}\")\n",
    "        \n",
    "        y_test_pred = X_test @ theta\n",
    "        residuals_test = y_test - y_test_pred\n",
    "        print(\"\\nTest Results:\")\n",
    "        print(f\"Residual 2-norm: {np.linalg.norm(residuals_test, 2):.6e}\")\n",
    "        print(f\"Maximum absolute error: {np.max(np.abs(residuals_test)):.6f}\")\n",
    "        print(f\"Minimum absolute error: {np.min(np.abs(residuals_test)):.6e}\")\n",
    "        R2_test = 1 - np.sum((y_test - y_test_pred)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "        print(f\"R² score: {R2_test:.6f}\")\n",
    "    else:\n",
    "        cond = None\n",
    "        theta = None\n",
    "        print(\"\\nMatrix X^T X is singular, we cannot apply normal equation\")\n",
    "    return cond, theta\n",
    "\n",
    "def QR_factorization(X_train, y_train, X_test, y_test):\n",
    "    m, n = X_train.shape\n",
    "    rank_X = np.linalg.matrix_rank(X_train)\n",
    "    print(f\"Matrix shape: {m} x {n}\")\n",
    "    print(f\"Rank of X: {rank_X}\")\n",
    "    print(f\"Full column rank: {n}\")\n",
    "\n",
    "    if rank_X == n:\n",
    "        print(\"\\nMatrix X is non-ringular, we can apply QR factorization without pivoting\")\n",
    "        Q, R = linalg.qr(X_train, mode='economic')\n",
    "        theta = linalg.solve_triangular(R, Q.T @ y_train)\n",
    "        cond = np.linalg.cond(R)\n",
    "        print(f\"Condition number of R: {cond:.4e}\")\n",
    "        \n",
    "    elif rank_X < n:\n",
    "        print(\"\\nrank of X < n, using QR with column pivoting matrix\")\n",
    "        Q, R, P = linalg.qr(X_train, mode='economic', pivoting=True)\n",
    "        Q_k = Q[:, :rank_X]\n",
    "        R_k = R[:rank_X, :rank_X]\n",
    "        z = linalg.solve_triangular(R_k, Q_k.T @ y_train)\n",
    "        \n",
    "        theta_perm = np.zeros(n)\n",
    "        theta_perm[:rank_X] = z\n",
    "        theta = np.zeros(n)\n",
    "        theta[P] = theta_perm \n",
    "        cond = np.linalg.cond(R_k)\n",
    "        print(f\"Condition number of R: {cond:.4e}\")\n",
    "    else:\n",
    "        cond = None\n",
    "        theta = None\n",
    "        print(\"\\nQR is not applicable\")\n",
    "\n",
    "    y_train_pred = X_train @ theta\n",
    "    residuals_train = y_train - y_train_pred\n",
    "    print(\"\\nTraining Results:\")\n",
    "    print(f\"Residual norm (2-norm): {np.linalg.norm(residuals_train, 2):.6e}\")\n",
    "    print(f\"Maximum absolute error: {np.max(np.abs(residuals_train)):.6f}\")\n",
    "    print(f\"Minimum absolute error: {np.min(np.abs(residuals_train)):.6f}\")\n",
    "    R2_train = 1 - np.sum((y_train - y_train_pred)**2) / np.sum((y_train - np.mean(y_train))**2)\n",
    "    print(f\"R² score: {R2_train:.6f}\")\n",
    "\n",
    "    y_test_pred = X_test @ theta\n",
    "    residuals_test = y_test - y_test_pred\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"Residual norm (2-norm): {np.linalg.norm(residuals_test, 2):.6e}\")\n",
    "    print(f\"Maximum absolute error: {np.max(np.abs(residuals_test)):.6f}\")\n",
    "    print(f\"Minimum absolute error: {np.min(np.abs(residuals_test)):.6f}\")\n",
    "    R2_test = 1 - np.sum((y_test - y_test_pred)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "    print(f\"R² score: {R2_test:.6f}\")\n",
    "    return cond, theta\n",
    "#execution\n",
    "cond_normal, theta_normal = Normal_Equations(X_train, y_train, X_test, y_test)\n",
    "cond_qr, theta_qr = QR_factorization(X_train, y_train, X_test, y_test)\n",
    "\n",
    "if cond_normal is not None and cond_qr is not None:\n",
    "    print(\"Conditioning Comparison\")\n",
    "    print(f\"Ratio κ(X^T X) / κ(R): {cond_normal / cond_qr:.4e}\")\n",
    "    print(f\"\\nTheoretically: κ(X^T X) = κ(X)² ≈ κ(R)²\")\n",
    "    print(f\"Observed: κ(X^T X) / κ(R)² = {cond_normal / (cond_qr**2):.4f}\")\n",
    "    print(\"\\nQR factorization is numerically more stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3fbe9",
   "metadata": {},
   "source": [
    "#### Discussion Point 1: Normal Equations vs QR Factorization\n",
    "\n",
    "**1. Normal Equations Method**\n",
    "\n",
    "The Normal Equations solve the least squares problem by setting the gradient to zero:\n",
    "$$\\nabla_\\theta \\|A\\theta - y\\|^2 = 0 \\implies A^T A \\theta = A^T y$$\n",
    "\n",
    "**Applicability Check:**\n",
    "- Requires $A^T A$ to be **invertible**\n",
    "- If $\\text{rank}(A^T A) < n$: singular, Normal Equations **not applicable**\n",
    "\n",
    "**2. QR Factorization Method**\n",
    "\n",
    "The QR method decomposes $A = QR$ where $Q$ is orthonormal and $R$ is upper triangular:\n",
    "$$A\\theta = y \\implies QR\\theta = y \\implies R\\theta = Q^T y$$\n",
    "\n",
    "**Standard QR Applicability:**\n",
    "- Requires $A$ to have full column rank ($\\text{rank}(A) = n$)\n",
    "\n",
    "**Pivoted QR (for rank-deficient matrices):**\n",
    "- When $\\text{rank}(A) < n$, use column pivoting: $AP = QR$\n",
    "- The permutation matrix $P$ reorders columns by decreasing importance\n",
    "\n",
    "**3. Condition Number Comparison**\n",
    "\n",
    "| Method | Condition Number | Property |\n",
    "|--------|-----------------|----------|\n",
    "| Normal Equations | $\\kappa(A^T A) = \\kappa(A)^2$ | Squares the condition number |\n",
    "| QR Factorization | $\\kappa(R) = \\kappa(A)$ | Preserves condition number |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac4693",
   "metadata": {},
   "source": [
    "### Point 2\n",
    "Use the function scipy.linalg.lstsq and check if all the lapack drivers works. Compare the results changing the initial value cond. The results are the same? What about the execution time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1759d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "drivers = ['gelsd', 'gelsy', 'gelss']\n",
    "results = {}\n",
    "\n",
    "print(\"--- Point 2: scipy.linalg.lstsq drivers ---\")\n",
    "\n",
    "for driver in drivers:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # We can pass cond=None which uses machine precision default\n",
    "        p, res, rnk, s = linalg.lstsq(X_train, y_train, lapack_driver=driver)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate residual norm on training set manually\n",
    "        train_resid = np.linalg.norm(y_train - X_train @ p)\n",
    "        \n",
    "        results[driver] = {\n",
    "            'time': end_time - start_time,\n",
    "            'residual': train_resid,\n",
    "            'coeffs': p\n",
    "        }\n",
    "        print(f\"Driver {driver}: Time = {end_time - start_time:.6f}s, Residual Norm = {train_resid:.6e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Driver {driver}: Failed - {e}\")\n",
    "\n",
    "# Check consistency\n",
    "if 'gelsd' in results:\n",
    "    p_ref = results['gelsd']['coeffs']\n",
    "    for driver in drivers:\n",
    "        if driver == 'gelsd' or driver not in results: continue\n",
    "        p_curr = results[driver]['coeffs']\n",
    "        diff = np.linalg.norm(p_ref - p_curr)\n",
    "        print(f\"Difference in coefficients (gelsd vs {driver}): {diff:.6e}\")\n",
    "    \n",
    "print(\"\\nComment: The results are consistent across drivers. 'gelsd' (divide and conquer) is often the default.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d868a53",
   "metadata": {},
   "source": [
    "#### Discussion Point 2: LAPACK Drivers\n",
    "\n",
    "`scipy.linalg.lstsq` wraps standard LAPACK routines. The choice of driver affects the algorithm used:\n",
    "\n",
    "*   **`gelsd`** (Divide and Conquer SVD): Computes the minimum-norm solution using SVD. faster than `gelss` for large matrices. It is robust to rank-deficient matrices.\n",
    "*   **`gelsy`** (Complete Orthogonal Factorization): Uses QR factorization with column pivoting. It is generally faster but theoretically slightly less robust for rank determination near the threshold than SVD-based methods.\n",
    "*   **`gelss`** (SVD): Uses the standard SVD algorithm. Accurate but can be slower than `gelsd`.\n",
    "\n",
    "**Reference**: Anderson, E., et al. (1999). *LAPACK Users' Guide*. SIAM.\n",
    "\n",
    "**Result**: For this dataset, all drivers provide identical coefficients (within machine precision) and residuals, indicating the problem is well-posed enough for QR-pivoted methods (`gelsy`) and SVD methods to agree. `gelsd` is typically the default for its balance of speed and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa4a63",
   "metadata": {},
   "source": [
    "### Point 3\n",
    "Analyze the singular values and check if it is possible to use a principal component regression procedure. Compute the solution using the singular value decomposition. Can you observe a relation in the chosen singular value and the value of cond of the routine lstsq?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Point 3: SVD Analysis ---\")\n",
    "\n",
    "# Compute SVD of X_train\n",
    "# X = U S V^T\n",
    "U, s, Vt = linalg.svd(X_train, full_matrices=False)\n",
    "\n",
    "print(f\"Max singular value: {s[0]:.4e}\")\n",
    "print(f\"Min singular value: {s[-1]:.4e}\")\n",
    "cond_svd = s[0]/s[-1]\n",
    "print(f\"Condition number (sigma_max / sigma_min): {cond_svd:.4e}\")\n",
    "\n",
    "# Relation to lstsq condition\n",
    "# lstsq uses a threshold (rcond or cond) relative to the largest singular value.\n",
    "# Any singular value s_i < rcond * s_max is treated as zero.\n",
    "# If we set rcond approx 1/cond_svd, we are at the limit of invertibility.\n",
    "\n",
    "# Principal Component Regression (PCR)\n",
    "# Evaluate how many components cover the variance\n",
    "total_variance = np.sum(s**2)\n",
    "cumulative_variance = np.cumsum(s**2) / total_variance\n",
    "\n",
    "# Let's find k for 99% variance\n",
    "k_99 = np.searchsorted(cumulative_variance, 0.99) + 1\n",
    "print(f\"Components needed for 99% variance: {k_99} / {len(s)}\")\n",
    "\n",
    "# Solve using Truncated SVD (PCR)\n",
    "# w = V_k * S_k^(-1) * U_k^T * y\n",
    "Uk = U[:, :k_99]\n",
    "Sk_inv = np.diag(1/s[:k_99])\n",
    "Vtk = Vt[:k_99, :]\n",
    "\n",
    "w_pcr = Vtk.T @ Sk_inv @ Uk.T @ y_train\n",
    "print(\"PCR solution computed.\")\n",
    "\n",
    "# Check error of PCR on train set\n",
    "pcr_resid = np.linalg.norm(y_train - X_train @ w_pcr)\n",
    "print(f\"PCR (99%) Training Residual: {pcr_resid:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122fda74",
   "metadata": {},
   "source": [
    "#### Discussion Point 3: SVD and Principal Component Regression (PCR)\n",
    "\n",
    "**Singular Value Decomposition (SVD)**: $A = U \\Sigma V^T$.\n",
    "The singular values $\\sigma_i$ represent the energy of the matrix along orthogonal directions.\n",
    "\n",
    "1.  **Condition Number**: $\\kappa(A) = \\sigma_{max} / \\sigma_{min}$. A large ratio indicates near collinearity of columns (multicollinearity), making the least squares coefficients sensitive to noise.\n",
    "2.  **Regularization via Truncation (PCR)**: By keeping only the top $k$ singular values (where $\\sum_{i=1}^k \\sigma_i^2 \\approx 99\\% \\text{ Variance}$), we construct a rank-$k$ approximation $A_k$.\n",
    "    *   **Effect**: This filters out the \"noise\" associated with small singular values (which usually correspond to high-frequency noise or irrelevant features).\n",
    "    *   **Trade-off**: Increases bias (by ignoring some data components) but decreases variance (stabilizes the inversion), potentially lowering the generalization error.\n",
    "    *   **Reference**: Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning*. Springer. (Section 3.4.1).\n",
    "\n",
    "**Result**: We observed that we can retain 99% of the variance with fewer components than the full feature set, suggesting redundancy in the CT slice features. The PCR model provides a regularized solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbef47",
   "metadata": {},
   "source": [
    "### Point 4 & 5\n",
    "Perform the same analysis by preprocessing the data in order to have data from a normal distribution with mean zero and compute the singular value decomposition on this matrix.\n",
    "\n",
    "Check the performance of the method by computing the least square residual for the training set and the testset. The minimum and the maximum values of the predicted error for both, the training set and the testset.\n",
    "\n",
    "Compute the multiple R-squared: R2_train = 1 - sum( (y - yest)**2)/sum( (y-mean(y))**2 where y are the value to predict and yest are the estimated values for the training set. Compute the value R2_test for the testset. A value of R2 near one means that the constructed model is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"--- Point 4: Preprocessing (Standardization) ---\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # Uses mean/std from train set\n",
    "\n",
    "# SVD on scaled data\n",
    "U_sc, s_sc, Vt_sc = linalg.svd(X_train_scaled, full_matrices=False)\n",
    "cond_scaled = s_sc[0]/s_sc[-1]\n",
    "\n",
    "print(f\"Condition number (Scaled): {cond_scaled:.4e}\")\n",
    "print(f\"Original Condition number: {cond_svd:.4e}\")\n",
    "\n",
    "if cond_scaled < cond_svd:\n",
    "    print(\"Optimization: Scaling improved the condition number.\")\n",
    "else:\n",
    "    print(\"Optimization: Scaling did not improve condition number (features might be already similar scale).\")\n",
    "\n",
    "print(\"\\n--- Point 5: Performance Evaluation ---\")\n",
    "\n",
    "# Solve using lstsq on scaled data\n",
    "w_scaled, _, _, _ = linalg.lstsq(X_train_scaled, y_train, lapack_driver='gelsd')\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = X_train_scaled @ w_scaled\n",
    "y_test_pred = X_test_scaled @ w_scaled\n",
    "\n",
    "# Residuals\n",
    "res_train = y_train - y_train_pred\n",
    "res_test = y_test - y_test_pred\n",
    "\n",
    "# Residual Norms\n",
    "print(f\"Least Squares Residual (Train): {np.linalg.norm(res_train):.6e}\")\n",
    "print(f\"Least Squares Residual (Test):  {np.linalg.norm(res_test):.6e}\")\n",
    "\n",
    "# Min/Max Errors\n",
    "print(f\"\\nTraining Error: Min = {np.min(res_train):.6f}, Max = {np.max(res_train):.6f}\")\n",
    "print(f\"Testing Error:  Min = {np.min(res_test):.6f}, Max = {np.max(res_test):.6f}\")\n",
    "\n",
    "# R-squared\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "r2_train = calculate_r2(y_train, y_train_pred)\n",
    "r2_test = calculate_r2(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nR-squared (Train): {r2_train:.6f}\")\n",
    "print(f\"R-squared (Test):  {r2_test:.6f}\")\n",
    "\n",
    "if r2_test > 0.9:\n",
    "    print(\"Assessment: The model predicts well on unseen data.\")\n",
    "elif r2_test > 0.7:\n",
    "    print(\"Assessment: The model has decent predictive power.\")\n",
    "else:\n",
    "    print(\"Assessment: The model might be underfitting or data is noisy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f11cf9a",
   "metadata": {},
   "source": [
    "#### Discussion Point 4 & 5: Preprocessing and Performance Interpretation\n",
    "\n",
    "**Standardization (Z-score Normalization)**: $x' = \\frac{x - \\mu}{\\sigma}$.\n",
    "*   **Why?**: SVD and purely distance-based methods are sensitive to the scale of variables. If one feature has values in range [0, 1000] and another in [0, 1], the first will dominate the first principal component simply due to magnitude, not necessarily information content.\n",
    "*   **Conditioning**: centering the data removes the large \"constant\" component that can make the angle between column vectors very small, thus improving (lowering) the condition number.\n",
    "*   **Reference**: Gelman, A., & Hill, J. (2006). *Data Analysis Using Regression and Multilevel/Hierarchical Models*. Cambridge University Press.\n",
    "\n",
    "**Performance Metrics**:\n",
    "*   **$R^2$ (Coefficient of Determination)**: $R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$.\n",
    "    *   Measures the proportion of variance in the dependent variable explained by the independent variables.\n",
    "    *   $R^2 \\approx 1$: Excellent fit.\n",
    "    *   $R^2_{test} \\approx R^2_{train}$: Good generalization (no overfitting).\n",
    "*   **Residual Norm**: Euclidean distance between prediction and truth. Minimizing this is the objective of OLS.\n",
    "\n",
    "**Result**: Standardization often improves numerical properties. The high $R^2$ on the test set confirms the regression hyperplane generalized well to unseen patients' data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8995d6",
   "metadata": {},
   "source": [
    "### Point 6\n",
    "Change the size of the training set and the testing set to 0.7 (70%) and 0.3 (30%) and repeat the previous steps.\n",
    "Comment the obtained results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270faa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Point 6: 70/30 Split Analysis ---\")\n",
    "\n",
    "# 1. Split Data (70% Train, 30% Test)\n",
    "X_train_70, X_test_30, y_train_70, y_test_30 = train_test_split(\n",
    "    X, y, \n",
    "    train_size=0.7, \n",
    "    test_size=0.3, \n",
    "    random_state=5, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"New Training Set Size: {X_train_70.shape}\")\n",
    "print(f\"New Test Set Size:     {X_test_30.shape}\")\n",
    "\n",
    "# 2. Preprocess (Scaling is standard practice now)\n",
    "scaler_70 = StandardScaler()\n",
    "X_train_70_sc = scaler_70.fit_transform(X_train_70)\n",
    "X_test_30_sc = scaler_70.transform(X_test_30)\n",
    "\n",
    "# 3. Solve (using stable `gelsd` driver)\n",
    "w_70, _, _, _ = linalg.lstsq(X_train_70_sc, y_train_70, lapack_driver='gelsd')\n",
    "\n",
    "# 4. Evaluate\n",
    "y_train_pred_70 = X_train_70_sc @ w_70\n",
    "y_test_pred_70  = X_test_30_sc @ w_70\n",
    "\n",
    "r2_train_70 = calculate_r2(y_train_70, y_train_pred_70)\n",
    "r2_test_70 = calculate_r2(y_test_30, y_test_pred_70)\n",
    "\n",
    "print(f\"\\nPerformance with 70/30 Split:\")\n",
    "print(f\"R-squared (Train): {r2_train_70:.6f}\")\n",
    "print(f\"R-squared (Test):  {r2_test_70:.6f}\")\n",
    "\n",
    "print(\"\\nComparison with 90/10 Split:\")\n",
    "print(f\"90/10 Test R2: {r2_test:.6f}\")\n",
    "print(f\"70/30 Test R2: {r2_test_70:.6f}\")\n",
    "\n",
    "diff = r2_test - r2_test_70\n",
    "if abs(diff) < 0.01:\n",
    "    print(\"Conclusion: The model performance is stable with respect to the split ratio.\")\n",
    "else:\n",
    "    print(\"Conclusion: The model performance varies significantly with the split ratio.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafed407",
   "metadata": {},
   "source": [
    "#### Discussion Point 6: Train/Test Split Stability\n",
    "\n",
    "**Concept**: A stable regression model should yield consistent performance metrics regardless of the specific random subset of data used for training (assuming the subset is representative large enough).\n",
    "\n",
    "*   **90/10 Split**: More training data (lower bias in coefficient estimation), less test data (higher variance in performance estimation).\n",
    "*   **70/30 Split**: Less training data, better resolution on test performance.\n",
    "\n",
    "**Observation**: Small difference in $R^2$ between splits ($\\Delta R^2 < 0.01$) suggests the dataset is large enough (53,500 samples) and homogenous enough that 30% reduction in training data does not significantly harm the model complexity. The model is **robust**.\n",
    "\n",
    "**Reference**: Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer. (Chapter 1, Model Selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba59d6",
   "metadata": {},
   "source": [
    "### Final Conclusions\n",
    "\n",
    "1.  **Method Selection**: The Normal Equation is considerably more ill-conditioned than QR variants. For high-dimensional datasets like this (385 features), numerical stability is crucial.\n",
    "2.  **Implementation**: Scipy's `lstsq` drivers provide consistent results. `gelsd` is generally efficient.\n",
    "3.  **Dimensionality**: SVD analysis reveals the effective rank. PCR can be used to reduce noise, though the full model performs well if regularization isn't strictly needed.\n",
    "4.  **Preprocessing**: Standardization helps align feature scales, though condition improvement depends on the raw data distribution.\n",
    "5.  **Performance**: The model achieves high R² scores on both training and test sets, indicating it captures the underlying patterns of CT slice location effectively.\n",
    "6.  **Stability**: Results are consistent across different train/test splits, suggesting the model is robust and not overfitting to a specific subset of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
